# Pipeline configuration (overrides defaults in pipeline.config.PipelineConfig)
# Edit these values to change pipeline behavior without modifying code.

days: 365
slide_weeks: 1
min_weeks: 4
max_weeks: 12
up_to: 20
grow_weeks: 1
top_n: 10
delay: 0.5
processes: 30  # Use 30 of 32 cores (leave 2 for system overhead)
max_combinations: 200000
# CPU batching knobs (used when running CPU-only optimized path)
cpu_batch_size: 1024      # Increased for better throughput
cpu_top_k: 256            # Increased to pass more to GPU (if enabled)
concurrency: 30           # Match process count for consistency
out_dir: output

# Pattern scanning mode: 'all' scans impulsive + corrective, 'impulses' scans only impulsive waves
scan_pattern_types: all

# Multi-start search: try multiple pivot points (local extrema) as wave starts
# Set to true to enable, false for single-start (faster)
enable_multi_start: true
max_start_points: 5

# Overlapping windows: set overlap_ratio > 0 to create overlapping time windows
# overlap_ratio=0.5 means 50% overlap (catches patterns spanning boundaries)
window_overlap_ratio: 0.3

# Enable shared memory for quick testing (workers will map lows/highs/dates)
use_shared_memory: true

# For quick profiling runs, lower the window budget so the run is fast. Set to 50 for quick test.
max_windows: 500

# New knobs
save_images: false
save_images_top_n: 1
chunk_size: 0
min_volatility: 0.0
skip_flat_windows: false
profile: true

# Pre-score knobs (cheap vectorized features to prune windows)
pre_score_top_k: 0
pre_score_threshold: 0.0
# weights: volatility, range, extrema_count, abs(slope)
pre_score_weights: [0.4, 0.3, 0.2, 0.1]

# Sweep knobs (optional) â€” used by `tools/sweep_cpu_batch.py` if present.
# Edit these lists to control which values are tried by the sweep driver.
cpu_batch_sizes: [128, 256, 512, 1024]
cpu_top_k_values: [16, 32, 64, 128]

# Suggested profiles (examples):
# - Local Apple Silicon (M4 Pro):
#   processes: 6           # use physical cores or cores-1
#   concurrency: 12        # network fetch parallelism
#   chunk_size: 16         # tune for your workload
#   max_windows: 500       # cap per-symbol windows for exploratory runs
#   save_images: false
#
# - HPC (A100 + 32 threads):
#   processes: 31          # use number of CPU workers (threads) you want to utilize
#   concurrency: 64        # many async fetchers / HF readers
#   chunk_size: 64         # larger chunk size reduces IPC overhead on high-core machines
#   max_windows: 2000      # increase scan budget if you want exhaustive search
#   save_images: false     # keep off for bulk runs; enable only top_n small

