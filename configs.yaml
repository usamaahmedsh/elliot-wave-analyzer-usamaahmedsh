# Pipeline configuration optimized for HPC (16 CPU cores + 12GB NVIDIA GPU)
# High-performance settings for cluster computing

# AUTO DEVICE DETECTION (recommended)
# Set to true to automatically detect hardware and optimize settings
# - Detects CUDA GPUs (HPC/NVIDIA)
# - Detects Apple Silicon GPU (Mac M1/M2/M3/M4)
# - Falls back to optimized CPU settings
# When enabled, automatically sets: processes, cpu_batch_size, concurrency
auto_detect_device: true

# Historical data range
# days: 5475   = 15 years of data
# days: 7300   = 20 years of data
# days: 0      = ALL available history (yfinance "max" period)
days: 5475  # 15 years of data
slide_weeks: 2  # Step size for window sliding
min_weeks: 8  # Minimum 8 weeks - reasonable pattern length
max_weeks: 104  # 2 years max window - good for most Elliott Wave patterns
up_to: 8  # Moderate complexity - good balance of depth vs speed
grow_weeks: 2
top_n: 20  # Keep top 20 patterns per window
delay: 0.5

# Optimized for HPC (16 CPU cores + 12GB NVIDIA GPU)
# These will be AUTO-DETECTED if auto_detect_device=true
# Only used if auto_detect_device=false
processes: 14  # Use 14 workers (leave 2 cores for system overhead)
cpu_batch_size: 2048  # Larger batch size for dedicated GPU VRAM
concurrency: 16  # Network fetch parallelism - match CPU cores

max_combinations: 500000  # 500K combinations - good coverage without excessive runtime
out_dir: output

# Pattern scanning mode: 'all' scans impulsive + corrective, 'impulses' scans only impulsive waves
scan_pattern_types: all  # Scan both impulsive and corrective patterns

# Multi-start search: try multiple pivot points (local extrema) as wave starts
# Set to true to enable, false for single-start (faster)
enable_multi_start: true  # Enable to find patterns at different starting points
max_start_points: 8  # Try 8 starting points - balanced coverage

# Overlapping windows: set overlap_ratio > 0 to create overlapping time windows
# overlap_ratio=0.5 means 50% overlap (catches patterns spanning boundaries)
window_overlap_ratio: 0.5  # 50% overlap - good balance

# Enable shared memory for efficient multi-processing
use_shared_memory: true

# Window budget per symbol
max_windows: 500  # Scan 500 windows per symbol - reasonable coverage

# Image saving - ENABLED for manual review
save_images: true  # Save pattern visualizations
save_images_top_n: 5  # Save top 5 patterns per symbol

# Inline pattern analysis - show quality statistics during pipeline run
analyze_patterns: true  # Set to true to see score distributions per symbol in real-time
chunk_size: 0
min_volatility: 0.0
skip_flat_windows: false
profile: false  # Disable profiling for production runs

# Pre-score knobs (cheap vectorized features to prune windows)
pre_score_top_k: 0
pre_score_threshold: 0.0
# weights: volatility, range, extrema_count, abs(slope)
pre_score_weights: [0.4, 0.3, 0.2, 0.1]

# Sweep knobs (optional) â€” used by `tools/sweep_cpu_batch.py` if present.
# Edit these lists to control which values are tried by the sweep driver.
cpu_batch_sizes: [1024, 2048, 4096]
cpu_top_k_values: [128, 256, 512]

# HPC PROFILE (16 CPU cores + 12GB NVIDIA GPU):
# Expected output per symbol: 15-50 patterns
# Expected 315 symbols total: ~5,000 - 15,000 patterns
# Runtime per symbol: ~30-60 seconds (faster with CUDA)
# Full 315 symbols: ~3-5 hours
# Output file: 50-200 MB

