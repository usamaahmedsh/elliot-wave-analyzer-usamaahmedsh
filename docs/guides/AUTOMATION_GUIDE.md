# End-to-End Pipeline Automation

This guide explains how to run the complete Elliott Wave Analyzer pipeline with **zero manual setup**.

## üöÄ Quick Start (30 seconds)

The fastest way to get started:

```bash
chmod +x quickstart.sh
./quickstart.sh
```

This will:
1. ‚úÖ Create virtual environment (if needed)
2. ‚úÖ Install all dependencies (if needed)
3. ‚úÖ Run pattern detection on AAPL
4. ‚úÖ Evaluate results
5. ‚úÖ Save output to `output/quickstart_results.json`

---

## üìã Full Automated Pipeline

For production runs with multiple symbols and full evaluation:

```bash
chmod +x run_pipeline.sh
./run_pipeline.sh AAPL,MSFT,GOOG,TSLA
```

### What It Does

The `run_pipeline.sh` script is a **fully automated end-to-end pipeline** that:

1. **Environment Setup**
   - ‚úÖ Checks Python version (3.9+ required)
   - ‚úÖ Creates virtual environment (if not exists)
   - ‚úÖ Installs all dependencies from `requirements.txt`
   - ‚úÖ Optionally installs GPU acceleration (with `--gpu` flag)

2. **Dependency Verification**
   - ‚úÖ Verifies all critical packages are installed
   - ‚úÖ Shows version numbers for debugging
   - ‚úÖ Fails fast if dependencies missing

3. **Performance Optimization**
   - ‚úÖ Pre-warms Numba JIT functions (eliminates first-run overhead)
   - ‚úÖ Optionally detects and configures GPU acceleration

4. **Pattern Detection**
   - ‚úÖ Runs the wave analyzer pipeline
   - ‚úÖ Detects Elliott Wave patterns
   - ‚úÖ Saves results to timestamped output directory

5. **Evaluation**
   - ‚úÖ Validates rule compliance
   - ‚úÖ Calculates predictive metrics
   - ‚úÖ Generates HTML report and JSON metrics
   - ‚úÖ Creates summary file

6. **Results Storage**
   - ‚úÖ All outputs saved to `output/YYYYMMDD_HHMMSS/`
   - ‚úÖ Organized directory structure
   - ‚úÖ Comprehensive logging

---

## üìñ Usage Examples

### Basic Usage

```bash
# Run with default symbols (AAPL, MSFT, GOOG)
./run_pipeline.sh

# Run with custom symbols
./run_pipeline.sh AAPL,TSLA,NVDA

# Run with different config
./run_pipeline.sh --config configs_high_perf.yaml

# Quick test (single symbol)
./run_pipeline.sh --quick
```

### Advanced Options

```bash
# Enable GPU acceleration
./run_pipeline.sh --gpu

# Custom output directory
./run_pipeline.sh --output my_results

# Skip evaluation (pattern detection only)
./run_pipeline.sh --skip-eval

# High-performance config with GPU
./run_pipeline.sh AAPL,MSFT,GOOG,TSLA,NVDA --config configs_high_perf.yaml --gpu
```

### Full Command Reference

```
Usage: ./run_pipeline.sh [SYMBOLS] [OPTIONS]

Arguments:
  SYMBOLS              Comma-separated symbols (default: AAPL,MSFT,GOOG)

Options:
  --config FILE        Config file to use (default: configs.yaml)
  --output DIR         Output directory (default: output/TIMESTAMP)
  --gpu                Install GPU acceleration (CuPy)
  --skip-eval          Skip evaluation step
  --quick              Quick mode (single symbol AAPL)
  --help               Show help
```

---

## üìÅ Output Structure

After running the pipeline, you'll get a timestamped directory with all outputs:

```
output/20260215_143022/
‚îú‚îÄ‚îÄ results.json              # Detected patterns (main output)
‚îú‚îÄ‚îÄ evaluation_report.html    # Interactive evaluation report
‚îú‚îÄ‚îÄ evaluation_metrics.json   # Metrics data
‚îú‚îÄ‚îÄ summary.txt               # Quick summary
‚îú‚îÄ‚îÄ pipeline.log              # Detailed execution log
‚îî‚îÄ‚îÄ temp_config.yaml          # Config used for this run
```

### Output Files Explained

| File | Description | Use Case |
|------|-------------|----------|
| `results.json` | All detected patterns with scores | Load for analysis, visualization |
| `evaluation_report.html` | Interactive HTML report | Open in browser for visual analysis |
| `evaluation_metrics.json` | Precision, recall, F1, compliance | Automated quality checks |
| `summary.txt` | Quick stats and next steps | Quick reference |
| `pipeline.log` | Complete execution log | Debugging, performance analysis |

---

## üîß Configuration

### Using High-Performance Config

For maximum speed on powerful hardware (GPU + 32 cores):

```bash
./run_pipeline.sh --config configs_high_perf.yaml --gpu
```

This enables:
- ‚úÖ 30 parallel workers (utilize all CPU cores)
- ‚úÖ GPU batch scoring (20,000 candidates per batch)
- ‚úÖ Optimized batch sizes
- ‚úÖ Maximum concurrency
- ‚úÖ **20-50x speedup** vs default config

### Custom Configuration

You can modify configs in `configs.yaml` or `configs_high_perf.yaml`:

```yaml
# CPU parallelism
processes: 30              # Number of worker processes
cpu_batch_size: 1024       # Candidates per CPU batch
concurrency: 30            # Parallel data downloads

# GPU acceleration
use_gpu: true              # Enable GPU (requires CuPy)
gpu_batch_size: 20000      # Candidates per GPU batch

# Pattern detection
patterns: [impulse, zigzag]
structure: [5, 3]
max_windows: 2000
```

---

## ‚ö° Performance Modes

### Quick Mode (Testing)
```bash
./run_pipeline.sh --quick
```
- Single symbol (AAPL)
- Default config
- ~30-60 seconds
- Good for: Testing, development

### Standard Mode (Production)
```bash
./run_pipeline.sh AAPL,MSFT,GOOG,TSLA
```
- Multiple symbols
- Default config
- ~5-10 minutes
- Good for: Daily analysis

### High-Performance Mode (Production)
```bash
./run_pipeline.sh AAPL,MSFT,GOOG,TSLA,NVDA --config configs_high_perf.yaml --gpu
```
- Multiple symbols
- Optimized config + GPU
- ~1-2 minutes (20-50x faster)
- Good for: Large-scale analysis, backtesting

---

## üêõ Troubleshooting

### Virtual Environment Issues

**Problem**: Virtual environment creation fails
```bash
# Solution: Ensure Python 3.9+ is installed
python3 --version

# If needed, install Python 3.11
brew install python@3.11  # macOS
```

**Problem**: Permission denied
```bash
# Solution: Make scripts executable
chmod +x run_pipeline.sh quickstart.sh
```

### Dependency Issues

**Problem**: Dependencies fail to install
```bash
# Solution 1: Upgrade pip
source .venv/bin/activate
python -m pip install --upgrade pip

# Solution 2: Install dependencies manually
pip install -r requirements.txt --verbose
```

**Problem**: Numba installation fails
```bash
# Solution: Install system dependencies
brew install llvm  # macOS
# Then reinstall numba
pip install --force-reinstall numba
```

### GPU Issues

**Problem**: GPU not detected
```bash
# Check if NVIDIA GPU is available
nvidia-smi

# Check CUDA version
nvcc --version
```

**Problem**: CuPy installation fails
```bash
# Install for CUDA 12.x
pip install cupy-cuda12x

# Or for CUDA 11.x
pip install cupy-cuda11x
```

### Pipeline Errors

**Problem**: Pipeline fails during execution
```bash
# Check detailed log
cat output/TIMESTAMP/pipeline.log

# Run with verbose logging
./run_pipeline.sh --quick  # Test with single symbol
```

**Problem**: Out of memory
```bash
# Solution: Reduce batch sizes in config
# Edit configs.yaml:
cpu_batch_size: 512        # Reduce from 1024
max_windows: 1000          # Reduce from 2000
```

---

## üéØ Next Steps After Running

### 1. View Results

```bash
# Quick summary
cat output/TIMESTAMP/summary.txt

# View all patterns (requires jq)
cat output/TIMESTAMP/results.json | jq '.'

# Count patterns
cat output/TIMESTAMP/results.json | jq '.patterns | length'
```

### 2. Open Evaluation Report

```bash
# macOS
open output/TIMESTAMP/evaluation_report.html

# Linux
xdg-open output/TIMESTAMP/evaluation_report.html
```

### 3. Analyze Metrics

```bash
# View evaluation metrics
cat output/TIMESTAMP/evaluation_metrics.json | jq '.'

# Check rule compliance
cat output/TIMESTAMP/evaluation_metrics.json | jq '.rules.validation_rate'

# Check pattern counts
cat output/TIMESTAMP/evaluation_metrics.json | jq '.rules.stats'
```

### 4. Re-run with Different Config

```bash
# Try high-performance config
./run_pipeline.sh --config configs_high_perf.yaml

# Compare results
diff output/run1/summary.txt output/run2/summary.txt
```

---

## üìä Expected Performance

| Mode | Symbols | Time | Speedup |
|------|---------|------|---------|
| Quick | 1 | ~30-60s | Baseline |
| Standard | 3-5 | ~5-10min | 1x |
| High-Perf (CPU) | 3-5 | ~2-4min | 5x |
| High-Perf (GPU) | 3-5 | ~1-2min | 20-50x |

*Performance varies based on hardware and data size*

---

## üîÑ Integration with CI/CD

The automated pipeline can be integrated into CI/CD workflows:

```yaml
# Example GitHub Actions workflow
name: Run Elliott Wave Analysis

on:
  schedule:
    - cron: '0 0 * * *'  # Daily at midnight

jobs:
  analyze:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Setup Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.11'
      - name: Run Pipeline
        run: |
          chmod +x run_pipeline.sh
          ./run_pipeline.sh AAPL,MSFT,GOOG --skip-eval
      - name: Upload Results
        uses: actions/upload-artifact@v2
        with:
          name: wave-patterns
          path: output/
```

---

## üìù Summary

The automated pipeline provides:

‚úÖ **Zero-config setup** - Just run the script  
‚úÖ **Automatic dependency management** - Virtual env + all packages  
‚úÖ **GPU auto-detection** - Uses GPU if available  
‚úÖ **Complete evaluation** - Rules, metrics, reports  
‚úÖ **Organized output** - Timestamped directories  
‚úÖ **Comprehensive logging** - Debug-friendly logs  
‚úÖ **Flexible configuration** - Multiple performance modes  
‚úÖ **Production-ready** - Suitable for automation  

**Two commands to get started:**
```bash
chmod +x quickstart.sh run_pipeline.sh
./quickstart.sh                    # Quick test
./run_pipeline.sh                  # Full pipeline
```

For questions or issues, see the troubleshooting section or check the logs.
